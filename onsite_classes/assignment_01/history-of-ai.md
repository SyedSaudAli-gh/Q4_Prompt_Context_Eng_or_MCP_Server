---
marp: true
title: The History of AI â€” From Symbolic Systems to Agentic AI
paginate: true
header: 'History of AI'
footer: 'Â© Syed Saud Ali'
theme: default
---

![width:1200px h:600px center](images/history-of-ai.png)

# ğŸ§  The History of AI  
From Symbolic Systems to Agentic AI

- Evolution, key milestones, paradigm shifts  
- How LLMs work + breakthroughs  
- From LLMs to Agentic AI  

---

## ğŸ“‹ Agenda
1. Foundations (1940sâ€“1956)  
2. Symbolic AI â†’ Expert Systems  
3. Statistical ML (1990sâ€“2000s)  
4. Deep Learning â†’ Transformer  
5. LLMs: How They Work + Alignment  
6. Agentic AI, Risks, and Whatâ€™s Next  

---

## ğŸ›ï¸ Foundations (1940sâ€“1956)
- 1943: McCulloch & Pitts â€” Neural nets as logic  
- 1950: Alan Turing â€” *The Imitation Game*  
- Shannon & Wiener â€” Information theory, cybernetics  
- 1956: Dartmouth Workshop â€” Term â€œArtificial Intelligenceâ€ coined  

---

## ğŸ§© Symbolic Era (1956â€“1969)
- GOFAI: Logic, search, and planning  
- Systems: Logic Theorist, GPS, ELIZA, SHRDLU  
- Perceptron (Rosenblatt): Early neural optimism  
- **Limits:** Brittleness, combinatorial explosion  

---

## â„ï¸ First AI Winter (1970s)
- 1969: Minsky & Papert critique Perceptrons  
- 1973: Lighthill Report â†’ Funding cuts  
- Limited data and compute â†’ research slowdown  

---

## âš™ï¸ Expert Systems (1970sâ€“1980s)
- Knowledge-based systems, rule inference  
- Examples: MYCIN, DENDRAL, XCON  
- **Pros:** Narrow domain success  
- **Cons:** Brittle, costly to maintain, poor scalability  

---

## ğŸ“Š Statistical ML (1990sâ€“2000s)
- Bayesian Networks, HMMs, EM Algorithm  
- SVMs, Decision Trees, Ensembles  
- Reinforcement Learning: TD, Q-learning, Policy Gradient  
- **Applications:** Speech, OCR, Recommendation systems  

---

## ğŸ¤– Deep Learning Revival (2006â€“2016)
- Deep Belief Networks (2006), GPUs, Big Data  
- Techniques: ReLU, Dropout, BatchNorm  
- **ImageNet 2012:** AlexNet breakthrough  
- Seq2Seq, Attention, ResNet architectures  

---

## âš¡ The Transformer (2017)
- *â€œAttention Is All You Needâ€*  
- Introduced Self-Attention + Multi-Head mechanisms  
- Enables parallel sequence modeling  
- Foundation for GPT, BERT, T5, and LLaMA  

---

## ğŸ”„ Pretraining & Transfer (2018â€“2020)
- Models: ELMo, BERT (masked LM), GPT (generative pretrain)  
- Fine-tuning â†’ task transfer without retraining  
- Marked start of **generalized NLP capabilities**  

---

## ğŸš€ Scaling to LLMs (2020â€“2023)
- GPT-3: Few-shot behavior emerges  
- Scaling laws & Chinchilla data/compute tradeoff  
- Instruction tuning, RLHF â†’ chat models  
- Open-source wave: LLaMA, Falcon, Mistral  

---

## ğŸ§  How LLMs Work
- Objective: **Next-token prediction**  
- Architecture: Transformer (decoder or encoderâ€“decoder)  
- Training: Self-supervised on web-scale text  
- Post-training: SFT, RLHF/DPO, safety tuning  
- Inference: Sampling, KV cache, quantization  

---

## âš™ï¸ LLM Pipeline
![width:900px center](images/llm-pipeline.png)

---

## ğŸ” Inside a Transformer Block
![width:500px center](images/transformer-block.png)

**Flow:** Embeddings â†’ Self-Attention â†’ Residuals â†’ MLP â†’ Residuals  

---

## ğŸ¤ Alignment
![width:500px center](images/alignment-flow.png)

**Stages:** SFT â†’ Reward Model â†’ RLHF/DPO â†’ Aligned Chat Model  

---

## ğŸ§© Retrieval-Augmented Generation (RAG)
![width:500px center](images/rag-workflow.png)

Retriever injects **fresh, factual context** to reduce hallucinations.  

---

## ğŸ§­ Agentic AI
![width:600px center](images/agentic-loop.png)

**Agents = LLMs + Tools + Memory + Goals**  
They can plan, reason, and act autonomously to achieve objectives.  

---

## ğŸŒŸ Breakthroughs Enabling LLMs
![width:600px center](images/llm-enablers.png)

**Key Enablers:**  
- Scalable Transformer Architectures  
- Massive Data + Compute (GPUs/TPUs)  
- Efficient Training (Mixed Precision, Checkpointing)  
- Reinforcement Learning from Human Feedback (RLHF)  

---

## âš–ï¸ Opportunities and Risks
- **Opportunities:** Productivity, discovery, accessibility  
- **Risks:** Hallucinations, bias, misuse, IP/privacy  
- **Mitigations:** RAG, guardrails, evaluations, monitoring  

---

## ğŸ”® Whatâ€™s Next
- Smarter tool use & planning (robust agents)  
- Personalization with privacy  
- Multimodal reasoning (vision, audio, action)  
- On-device AI via quantization & MoE models  

---

## ğŸ§¾ Summary
- **AIâ€™s journey:** From symbolic logic â†’ neural learning â†’ LLMs â†’ Agents  
- **LLMs work:** Predict next tokens using Transformer architecture  
- **Breakthroughs:** Scaling, data, architecture, RLHF  
- **Future:** Agentic, personalized, multimodal AI  

---

# ğŸ™‹â€â™‚ï¸ Q&A
Thanks for listening!  
Letâ€™s discuss how Agentic AI will reshape the future.
